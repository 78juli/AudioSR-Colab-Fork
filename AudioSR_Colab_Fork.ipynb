{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AudioSR-Colab-Fork v0.2\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Colab adaptation of AudioSR, with some tweaks:\n",
        "- added a chunking feature to process input of any length\n",
        "- added stereo handling, stereo input channels will be splitted and processed independantly (dual mono) and then reconstructed as stereo audio.\n",
        "- added overlap feature to smooth the transitions between chunks (**don't use high values because AudioSR is not 100% phase accurate and this will create weird phase cancellation in the overlapping regions**)\n",
        "\n",
        "---\n",
        "Adaptation & tweaks by [jarredou](https://https://github.com/jarredou/)\n",
        "\n",
        "Original work [AudioSR: Versatile Audio Super-resolution at Scale](https://github.com/haoheliu/versatile_audio_super_resolution) by Haohe Liu, Ke Chen, Qiao Tian, Wenwu Wang, Mark D. Plumbley\n",
        "\n"
      ],
      "metadata": {
        "id": "bYuiV2rlDizI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t23Tz5XIcON",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Installation\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/haoheliu/versatile_audio_super_resolution.git\n",
        "%cd versatile_audio_super_resolution\n",
        "!pip install -r requirements.txt\n",
        "!pip install cog huggingface_hub unidecode phonemizer einops torchlibrosa transformers ftfy timm librosa\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPORTANT NOTE**\n",
        "\n",
        "#### If the inference cell crashes, restart the runtime (do not disconnect, just restart it), else it will cause memory errors !\n",
        "\n",
        "*If you're are doing multiple runs, think also to restart the runtime every 4 or 5 files to clean up memory*"
      ],
      "metadata": {
        "id": "gWUd27QECiLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #Inference\n",
        "input_file_path = '/content/versatile_audio_super_resolution/test_LR_30s.wav' #@param {type:\"string\"}\n",
        "output_folder = '/content/drive/MyDrive/output' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "ddim_steps= 50 #@param {type:\"slider\", min:20, max:200, step:10}\n",
        "overlap = 0.04 #@param {type:\"slider\", min:0, max:0.96, step:0.04}\n",
        "guidance_scale=3.5 #@param {type:\"slider\", min:1, max:15, step:0.5}\n",
        "seed = 0 # @param {type:\"integer\"}\n",
        "chunk_size = 10.24 # @param [\"5.12\", \"10.24\"] {type:\"raw\"}\n",
        "%cd /content/versatile_audio_super_resolution\n",
        "import gc\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.signal.windows import hann\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from cog import BasePredictor, Input, Path\n",
        "import tempfile\n",
        "import librosa\n",
        "from audiosr import build_model, super_resolution\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "class Predictor(BasePredictor):\n",
        "    def setup(self, model_name=\"basic\", device=\"auto\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.sr = 48000\n",
        "        print(\"Loading Model...\")\n",
        "        self.audiosr = build_model(model_name=self.model_name, device=self.device)\n",
        "        print(\"Model loaded !\")\n",
        "\n",
        "    def process_audio(self, input_file, chunk_size=5.12, overlap=0.1, seed=None, guidance_scale=3.5, ddim_steps=50):\n",
        "\n",
        "        # check if input_file is a file path or a numpy array\n",
        "        if isinstance(input_file, str):\n",
        "            audio, sr = sf.read(input_file)\n",
        "        else:\n",
        "            audio = input_file\n",
        "        print(f\"original samplerate = {sr}\")\n",
        "        # check if audio is stereo & split channels\n",
        "        is_stereo = len(audio.shape) == 2\n",
        "        if is_stereo:\n",
        "            print(\"audio is stereo\")\n",
        "            audio_ch1, audio_ch2 = audio[:, 0], audio[:, 1]\n",
        "        else:\n",
        "            print(\"Audio is mono\")\n",
        "            audio_ch1 = audio\n",
        "\n",
        "\n",
        "        # define chunk and overlap size in samples based on input sample rate\n",
        "        chunk_samples = int(chunk_size * sr)\n",
        "        overlap_samples = int(overlap * chunk_samples)\n",
        "\n",
        "        # calculate chunk size and overlap based on output sample rate\n",
        "        output_chunk_samples = int(chunk_size * self.sr)\n",
        "        output_overlap_samples = int(overlap * output_chunk_samples)\n",
        "        enable_overlap = True if overlap > 0 else False\n",
        "\n",
        "        print(f\"enable_overlap = {enable_overlap}\")\n",
        "        def process_chunks(audio):\n",
        "            chunks = []\n",
        "            original_lengths = []\n",
        "            start = 0\n",
        "            while start < len(audio):\n",
        "                end = min(start + chunk_samples, len(audio))\n",
        "                chunk = audio[start:end]\n",
        "                if len(chunk) < chunk_samples:\n",
        "                    original_lengths.append(len(chunk))\n",
        "                    pad = np.zeros(chunk_samples - len(chunk))\n",
        "                    chunk = np.concatenate([chunk, pad])\n",
        "                else:\n",
        "                    original_lengths.append(chunk_samples)\n",
        "                chunks.append(chunk)\n",
        "                start += chunk_samples - overlap_samples if enable_overlap else chunk_samples\n",
        "            return chunks, original_lengths\n",
        "\n",
        "        # create chunks lists for each channel\n",
        "        chunks_ch1, original_lengths_ch1 = process_chunks(audio_ch1)\n",
        "        if is_stereo:\n",
        "            chunks_ch2, original_lengths_ch2 = process_chunks(audio_ch2)\n",
        "\n",
        "        # process each chunk with the model and reconstruct the audio\n",
        "        sample_rate_ratio = self.sr / sr\n",
        "        total_length = len(chunks_ch1) * output_chunk_samples - (len(chunks_ch1) - 1) * (output_overlap_samples if enable_overlap else 0)\n",
        "        reconstructed_ch1 = np.zeros((1, total_length))\n",
        "        # print(reconstructed_ch1.shape)\n",
        "\n",
        "        for i, chunk in enumerate(chunks_ch1):\n",
        "            print(f\"Processing chunk {i+1} of {len(chunks_ch1)} for Left/Mono channel\")\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_wav:\n",
        "                sf.write(temp_wav.name, chunk, sr)\n",
        "                # print(f\"chunk.shape = {chunk.shape}\")\n",
        "\n",
        "                out_chunk = super_resolution(\n",
        "                    self.audiosr,\n",
        "                    temp_wav.name,\n",
        "                    seed=seed,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    ddim_steps=ddim_steps,\n",
        "                    latent_t_per_second=12.8\n",
        "                )\n",
        "                # remove all junk added by audiosr\n",
        "                # print(f\"out_chunk.shape = {out_chunk.shape}\")\n",
        "                out_chunk = out_chunk[0]\n",
        "                # print(f\"reshaped out_chunk.shape = {out_chunk.shape}\")\n",
        "                num_samples_to_keep = int(original_lengths_ch1[i] * sample_rate_ratio)\n",
        "                # print(f\"num_samples_to_keep : {num_samples_to_keep}\")\n",
        "                out_chunk = out_chunk[:, :num_samples_to_keep]\n",
        "\n",
        "\n",
        "                # apply crossfade if overlap is enabled\n",
        "                if enable_overlap:\n",
        "                    # calculate the actual overlap size for this chunk\n",
        "                    actual_overlap_samples = min(output_overlap_samples, num_samples_to_keep)\n",
        "\n",
        "                    # create fade-out and fade-in arrays of the correct size\n",
        "                    fade_out = np.linspace(1., 0., actual_overlap_samples)\n",
        "                    fade_in = np.linspace(0., 1., actual_overlap_samples)\n",
        "\n",
        "                    if i == 0:\n",
        "                        out_chunk[:, -actual_overlap_samples:] *= fade_out\n",
        "\n",
        "                    elif i < len(chunks_ch1) - 1:\n",
        "                        out_chunk[:, :actual_overlap_samples] *= fade_in\n",
        "                        out_chunk[:, -actual_overlap_samples:] *= fade_out\n",
        "\n",
        "                    else:\n",
        "                        out_chunk[:, :actual_overlap_samples] *= fade_in\n",
        "\n",
        "                # print(f\"out_chunk.shape : {out_chunk.shape}\")\n",
        "\n",
        "                start = i * (output_chunk_samples - output_overlap_samples if enable_overlap else output_chunk_samples)\n",
        "                end = start + out_chunk.shape[1]\n",
        "                reconstructed_ch1[0, start:end] += out_chunk.flatten()\n",
        "\n",
        "\n",
        "        if is_stereo:\n",
        "            reconstructed_ch2 = np.zeros((1, total_length))\n",
        "            # print(reconstructed_ch2.shape)\n",
        "\n",
        "            for i, chunk in enumerate(chunks_ch2):\n",
        "                print(f\"Processing chunk {i+1} of {len(chunks_ch2)} for Right channel\")\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_wav:\n",
        "                    sf.write(temp_wav.name, chunk, sr)\n",
        "                    # print(f\"chunk.shape = {chunk.shape}\")\n",
        "\n",
        "                    out_chunk = super_resolution(\n",
        "                        self.audiosr,\n",
        "                        temp_wav.name,\n",
        "                        seed=seed,\n",
        "                        guidance_scale=guidance_scale,\n",
        "                        ddim_steps=ddim_steps,\n",
        "                        latent_t_per_second=12.8\n",
        "                    )\n",
        "\n",
        "                    # remove all junk added by audiosr\n",
        "                    # print(f\"out_chunk.shape = {out_chunk.shape}\")\n",
        "                    out_chunk = out_chunk[0]\n",
        "                    # print(f\"reshaped out_chunk.shape = {out_chunk.shape}\")\n",
        "                    num_samples_to_keep = int(original_lengths_ch2[i] * sample_rate_ratio)\n",
        "                    # print(f\"num_samples_to_keep : {num_samples_to_keep}\")\n",
        "                    out_chunk = out_chunk[:, :num_samples_to_keep]\n",
        "\n",
        "                    # apply crossfade if overlap is enabled\n",
        "                    if enable_overlap:\n",
        "                        # calculate the actual overlap size for this chunk\n",
        "                        actual_overlap_samples = min(output_overlap_samples, num_samples_to_keep)\n",
        "\n",
        "                        # create fade-out and fade-in arrays of the correct size\n",
        "                        fade_out = np.linspace(1., 0., actual_overlap_samples)\n",
        "                        fade_in = np.linspace(0., 1., actual_overlap_samples)\n",
        "\n",
        "                        # no fadein for 1st chunk\n",
        "                        if i == 0:\n",
        "                            out_chunk[:, -actual_overlap_samples:] *= fade_out\n",
        "\n",
        "                        elif i < len(chunks_ch1) - 1:\n",
        "                            out_chunk[:, :actual_overlap_samples] *= fade_in\n",
        "                            out_chunk[:, -actual_overlap_samples:] *= fade_out\n",
        "\n",
        "                        # no fadeout for last  chunk\n",
        "                        else:\n",
        "                            out_chunk[:, :actual_overlap_samples] *= fade_in\n",
        "\n",
        "                    # print(f\"out_chunk.shape : {out_chunk.shape}\")\n",
        "\n",
        "                    start = i * (output_chunk_samples - output_overlap_samples if enable_overlap else output_chunk_samples)\n",
        "                    end = start + out_chunk.shape[1]\n",
        "                    reconstructed_ch2[0, start:end] += out_chunk.flatten()\n",
        "\n",
        "                reconstructed_audio = np.stack([reconstructed_ch1, reconstructed_ch2], axis=-1)\n",
        "        else:\n",
        "            reconstructed_audio = reconstructed_ch1\n",
        "\n",
        "        # print(f\"reconstructed_audio shape : {reconstructed_audio.shape}\")\n",
        "        return reconstructed_audio[0]\n",
        "\n",
        "    def predict(self,\n",
        "        input_file: Path = Input(description=\"Audio to upsample\"),\n",
        "        ddim_steps: int = Input(description=\"Number of inference steps\", default=50, ge=10, le=500),\n",
        "        guidance_scale: float = Input(description=\"Scale for classifier free guidance\", default=3.5, ge=1.0, le=20.0),\n",
        "        overlap: float = Input(description=\"overlap size\", default=0.1),\n",
        "        chunk_size: float = Input(description=\"chunksize\", default=5.12),\n",
        "        seed: int = Input(description=\"Random seed. Leave blank to randomize the seed\", default=None)\n",
        "    ) -> Path:\n",
        "\n",
        "        if seed == 0:\n",
        "            seed = random.randint(0, 2**32 - 1)\n",
        "        print(f\"Setting seed to: {seed}\")\n",
        "        print(f\"overlap = {overlap}\")\n",
        "        print(f\"guidance_scale = {guidance_scale}\")\n",
        "        print(f\"ddim_steps = {ddim_steps}\")\n",
        "        print(f\"chunk_size = {chunk_size}\")\n",
        "        print(f\"input file = {os.path.basename(input_file)}\")\n",
        "\n",
        "        waveform = self.process_audio(\n",
        "            input_file,\n",
        "            chunk_size=chunk_size,\n",
        "            overlap=overlap,\n",
        "            seed=seed,\n",
        "            guidance_scale=guidance_scale,\n",
        "            ddim_steps=ddim_steps\n",
        "        )\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        sf.write(f\"{output_folder}/SR_{os.path.basename(input_file)}\", data=waveform, samplerate=48000,  subtype=\"FLOAT\")\n",
        "        print(f\"file created: {output_folder}/SR_{os.path.basename(input_file)}\")\n",
        "        del self.audiosr\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = Predictor()\n",
        "    p.setup()\n",
        "    out = p.predict(\n",
        "        input_file_path,\n",
        "        ddim_steps=ddim_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        seed=seed,\n",
        "        chunk_size=chunk_size,\n",
        "        overlap=overlap\n",
        "    )\n",
        "\n",
        "    del p\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "WfjZ_Q0OIepR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}